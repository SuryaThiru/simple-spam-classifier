{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple spam classification\n",
    "**author:** Surya K\n",
    "\n",
    "**dataset info:** The famous enron email dataset is used for the spam classification task. The dataset used to train had about 6000 emails containing multiple sentences containing spam emails in 1:3 ratio. \n",
    "\n",
    "**usage:** notebook has code to train the data.There is a html embedded version of the model store in the repo which can also be used\n",
    "\n",
    " \n",
    " U can use the saved model to evaluate without training (requires spacy model to be loaded) :\n",
    "\n",
    "![demo](spam_classifier.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import spacy\n",
    "import joblib # save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load datasets into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "spam_dir = 'dataset/spam/'\n",
    "ham_dir = 'dataset/ham/'\n",
    "\n",
    "def getfilecontents(dir_path):\n",
    "    global cnt\n",
    "    contents = []\n",
    "    files = os.listdir(dir_path)\n",
    "    \n",
    "    for file in files:\n",
    "        with open(dir_path + file, 'r') as f:\n",
    "            content = f.read()\n",
    "            contents.append(content.strip())\n",
    "    \n",
    "    return contents\n",
    "\n",
    "ham = getfilecontents(ham_dir)\n",
    "spam = getfilecontents(spam_dir)\n",
    "\n",
    "data = ham + spam\n",
    "labels = [0] * len(ham) + [1] * len(spam) # 0 - not spam, 1 - spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Subject: re : issue\\nfyi - see note below - already done .\\nstella\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by stella l morris / hou / ect on 12 / 14 / 99 10 : 18\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\nfrom : sherlyn schumack on 12 / 14 / 99 10 : 06 am\\nto : stella l morris / hou / ect @ ect\\ncc : howard b camp / hou / ect @ ect\\nsubject : re : issue\\nstella ,\\nthis has already been taken care of . you did this for me yesterday .\\nthanks .\\nhoward b camp\\n12 / 14 / 99 09 : 10 am\\nto : stella l morris / hou / ect @ ect\\ncc : sherlyn schumack / hou / ect @ ect , howard b camp / hou / ect @ ect , stacey\\nneuweiler / hou / ect @ ect , daren j farmer / hou / ect @ ect\\nsubject : issue\\nstella ,\\ncan you work with stacey or daren to resolve\\nhc\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by howard b camp / hou / ect on 12 / 14 / 99 09 : 08\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\nfrom : sherlyn schumack 12 / 13 / 99 01 : 14 pm\\nto : howard b camp / hou / ect @ ect\\ncc :\\nsubject : issue\\ni have to create accounting arrangement for purchase from unocal energy at\\nmeter 986782 . deal not tracked for 5 / 99 . volume on deal 114427 expired 4 / 99 .',\n",
       " 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3], labels[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    data, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    '''\n",
    "    text: string to tokenize\n",
    "    special character removal -> lemmatization\n",
    "    '''\n",
    "    doc = nlp(text)\n",
    "    # preprocess during tokenizing\n",
    "    tokens = [token.lemma_ for token in doc \n",
    "              if not (token.is_stop or token.is_digit or token.is_quote or token.is_space\n",
    "                     or token.is_punct or token.is_bracket)]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before tokenization:\n",
      " Subject: re : issue\n",
      "fyi - see note below - already done .\n",
      "stella\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by stella l morris / hou / ect on 12 / 14 / 99 10 : 18\n",
      "am - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "from : sherlyn schumack on 12 / 14 / 99 10 : 06 am\n",
      "to : stella l morris / hou / ect @ ect\n",
      "cc : howard b camp / hou / ect @ ect\n",
      "subject : re : issue\n",
      "stella ,\n",
      "this has already been taken care of . you did this for me yesterday .\n",
      "thanks .\n",
      "howard b camp\n",
      "12 / 14 / 99 09 : 10 am\n",
      "to : stella l morris / hou / ect @ ect\n",
      "cc : sherlyn schumack / hou / ect @ ect , howard b camp / hou / ect @ ect , stacey\n",
      "neuweiler / hou / ect @ ect , daren j farmer / hou / ect @ ect\n",
      "subject : issue\n",
      "stella ,\n",
      "can you work with stacey or daren to resolve\n",
      "hc\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by howard b camp / hou / ect on 12 / 14 / 99 09 : 08\n",
      "am - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "from : sherlyn schumack 12 / 13 / 99 01 : 14 pm\n",
      "to : howard b camp / hou / ect @ ect\n",
      "cc :\n",
      "subject : issue\n",
      "i have to create accounting arrangement for purchase from unocal energy at\n",
      "meter 986782 . deal not tracked for 5 / 99 . volume on deal 114427 expired 4 / 99 .\n",
      "\n",
      "\n",
      "after tokenization:\n",
      " ['subject', 'issue', 'fyi', 'note', 'stella', 'forward', 'stella', 'l', 'morris', 'hou', 'ect', 'sherlyn', 'schumack', 'stella', 'l', 'morris', 'hou', 'ect', 'ect', 'cc', 'howard', 'b', 'camp', 'hou', 'ect', 'ect', 'subject', 'issue', 'stella', 'take', 'care', 'yesterday', 'thank', 'howard', 'b', 'camp', 'stella', 'l', 'morris', 'hou', 'ect', 'ect', 'cc', 'sherlyn', 'schumack', 'hou', 'ect', 'ect', 'howard', 'b', 'camp', 'hou', 'ect', 'ect', 'stacey', 'neuweiler', 'hou', 'ect', 'ect', 'daren', 'j', 'farmer', 'hou', 'ect', 'ect', 'subject', 'issue', 'stella', 'work', 'stacey', 'daren', 'resolve', 'hc', 'forward', 'howard', 'b', 'camp', 'hou', 'ect', 'sherlyn', 'schumack', 'pm', 'howard', 'b', 'camp', 'hou', 'ect', 'ect', 'cc', 'subject', 'issue', 'create', 'accounting', 'arrangement', 'purchase', 'unocal', 'energy', 'meter', 'deal', 'track', 'volume', 'deal', 'expire']\n"
     ]
    }
   ],
   "source": [
    "# testing tokenizer\n",
    "print('before tokenization:\\n', data[3])\n",
    "print('\\n\\nafter tokenization:\\n', tokenizer(data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=Tr...      vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('classifier', classifier)])\n",
    "pipe.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model validation accuracy: 0.9194068343004513\n"
     ]
    }
   ],
   "source": [
    "acc = pipe.score(X_test, Y_test)\n",
    "print('Model validation accuracy: ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "save_dir = 'spamclassifier.pkl'\n",
    "\n",
    "with open(save_dir, 'wb') as f:\n",
    "    joblib.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text):\n",
    "    pred = pipe.predict([text])[0]\n",
    "    \n",
    "    if pred:\n",
    "        print('It is spam!!')\n",
    "    else:\n",
    "        print('Not spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not spam\n"
     ]
    }
   ],
   "source": [
    "classify('hello, how are you? hope you are well')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is spam!!\n"
     ]
    }
   ],
   "source": [
    "classify('Buy our products at 50% offer. visit your nearest store now!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not spam\n"
     ]
    }
   ],
   "source": [
    "classify('Hey. I want to receive some guidance in a project do u think you can help me?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is spam!!\n"
     ]
    }
   ],
   "source": [
    "classify('You are one of the winners of the surprise lottery. reply your bank account numbers to get the money')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "positive examples were relatively skewed towards medicinal drug based advertisements in the dataset, still performs good"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "7c955338-8232-40a9-b7bc-6ecec7c6594b",
    "theme": {
     "a955e09e-9346-4167-aa87-a2160e454452": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "a955e09e-9346-4167-aa87-a2160e454452",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         34,
         34,
         34
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         256,
         256,
         256
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         66,
         175,
         250
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         256,
         256,
         256
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 5.25
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 4
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3.5
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       },
       "p": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Source Sans Pro",
       "font-size": 6
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
